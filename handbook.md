#### **一、标题**
**例："基于Linux 5.10内核的进程调度机制性能分析实验日志"**


#### **二、引言**
1. **背景理论**：
   Linux内核采用CFS（完全公平调度器）作为默认进程调度算法，通过vruntime（虚拟运行时间）实现进程公平调度。CFS通过红黑树管理可运行进程，优先级高的进程获得更短的vruntime增量（参考《Linux内核设计与实现》第4版，Robert Love, 2010）。 
2. **实验目的**： 
   验证CFS调度算法在不同负载场景下的公平性，分析进程优先级、CPU亲和力对调度延迟的影响。 
3. **预期结果**： 
   高优先级进程的平均响应时间应低于低优先级进程，绑定CPU的进程调度延迟低于非绑定进程。 


#### **三、实验方法** 
1. **设备与环境**： 
   - 硬件：Intel i7-11700K CPU（8核16线程）、16GB RAM
   - 系统：Ubuntu 20.04 LTS（内核版本5.10.0-102-generic）
   - 工具：`perf`性能分析工具、`taskset`进程绑定工具、`sysstat`系统监控工具

2. **实验步骤**：
   - **场景1：优先级对比实验**
     1. 使用`nice`命令创建3个不同优先级进程（nice值分别为-10、0、+10），运行CPU密集型任务（例：`stress-ng --cpu 1 --nice -10`）。
     2. 用`perf stat -e context-switches,cycles`监控各进程上下文切换次数和CPU周期。
   - **场景2：CPU亲和力实验**
     1. 用`taskset -c 0`将进程绑定到CPU核心0，对比非绑定进程的调度延迟（使用`latencytop`工具测量）。
   - **数据采集**： 
     每个场景重复3次，取平均值；记录`/proc/sched_debug`文件中的vruntime数据用于分析。


#### **四、实验结果**
1. **场景1：优先级对调度的影响**
   | nice值 | 平均响应时间（ms） | 上下文切换次数/秒 | CPU利用率（%） |
   |--------|--------------------|-------------------|----------------|
   | -10    | 12.5 ± 0.8         | 245               | 98.7           |
   | 0      | 28.3 ± 1.2         | 376               | 95.4           |
   | +10    | 56.7 ± 1.5         | 512               | 89.3           |

2. **场景2：CPU亲和力实验数据**  
   | 绑定状态   | 平均调度延迟（μs） | vruntime偏差（ms） |
   |------------|--------------------|--------------------|
   | 绑定CPU 0  | 45.2 ± 3.1         | 2.1 ± 0.5          |
   | 非绑定     | 78.6 ± 4.3         | 5.8 ± 0.9          |

3. **图表分析**
   **图1：不同nice值进程的响应时间对比**
   （横轴：nice值，纵轴：响应时间（ms），标注趋势线：响应时间随nice值增加呈指数增长）


#### **五、讨论**
1. **结果分析**：
   - 场景1中，nice值每降低10，响应时间缩短约56%，符合CFS“优先级越高，vruntime增量越小”的设计逻辑。上下文切换次数随nice值升高而增加，表明低优先级进程更易被调度抢占。
   - 场景2中，绑定CPU的进程调度延迟降低42.5%，因避免了跨CPU核心的缓存失效（cache invalidation），验证了CPU亲和力对减少调度开销的作用。
2. **理论联系**：  
   实验数据与《Linux内核调度器源码分析》（赵炯，2019）中“CFS通过动态调整时间片长度实现公平性”的结论一致。高优先级进程获得更大的时间片权重（如nice=-10时权重为1024，nice=+10时权重为102），导致响应时间差异。
3. **改进建议**：  
   - 增加多核负载均衡场景（使用`stress-ng --cpu 8`），分析`load_balance()`函数对调度公平性的影响；
   - 结合`ftrace`工具跟踪调度器关键函数（如`pick_next_task_cfs()`）的执行路径，定位性能瓶颈。


#### **六、结论** 
- 实验验证了Linux 5.10内核CFS调度算法的公平性：优先级每降低10，响应时间平均缩短56%，CPU亲和力可减少42.5%调度延迟。
- 高负载场景下，低优先级进程存在“饥饿”风险（如nice=+10时CPU利用率仅89.3%），需通过`SCHED_DEADLINE`实时调度类优化。


#### **七、参考文献**
1. Robert Love. (2010). Linux Kernel Development (3rd ed.). Pearson Education.
2. 赵炯. (2019). Linux内核完全注释（基于5.0内核）. 机械工业出版社.
3. Linux内核文档：《Scheduler Design Notes》（Documentation/scheduler/sched-design-CFS.txt）


#### **八、附录**

**附录中重点体现实验过程（步骤），标准是实验可复现。**

- **附录A：实验脚本**
  ```bash
  # 优先级测试脚本（priority_test.sh）
  nice -n -10 stress-ng --cpu 1 --timeout 300s &
  nice -n 0 stress-ng --cpu 1 --timeout 300s &
  nice -n 10 stress-ng --cpu 1 --timeout 300s &
  sleep 10s
  perf stat -e context-switches,cycles -p $(pgrep stress-ng)
  ```
- **附录B：内核参数配置**
  ```bash
  # 开启调度器调试日志
  echo 1 > /proc/sys/kernel/sched_schedstats
  echo 1 > /proc/sys/kernel/sched_debug
  ```
